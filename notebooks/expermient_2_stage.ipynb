{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2-Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from src.models.simple_3d_model import LinearModel\n",
    "from src.models.pose_resnet import BasicBlock, PoseResNet, Bottleneck\n",
    "import src.bug_dataset\n",
    "from src.eval.accuracies import keypoint_depth_pck, get_max_preds\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Code\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stick_bug(ax, points, vis, prediction=False):\n",
    "    limb_ranges=[[0,7],[8,14],[15,21],[22,28],[29,35],[36,42],[43,49],[53,56],[59,62]]\n",
    "    reduced_limb_ranges = [[0,4],[4,7],[7,10],[10,13],[13,16],[16,19],[19,22],[22,25],[25,28]]\n",
    "    if len(points) == 62:\n",
    "        points = np.array(points).T\n",
    "        for num in range(len(limb_ranges)):\n",
    "            visible_limb = []\n",
    "            for x in range(limb_ranges[num][0],limb_ranges[num][1]):\n",
    "                if vis[x]== 1:\n",
    "                    visible_limb.append(x) \n",
    "            if prediction:\n",
    "                line, = ax.plot(points[0][visible_limb], points[1][visible_limb], points[2][visible_limb],'--', alpha=0.7, label='Prediction', color='red')\n",
    "                ax.scatter(points[0][visible_limb], points[1][visible_limb], points[2][visible_limb], marker='X',s=10, color='red')\n",
    "            else:\n",
    "                line, = ax.plot(points[0][visible_limb], points[1][visible_limb], points[2][visible_limb], label='Actual', color='blue')\n",
    "                ax.scatter(points[0][visible_limb], points[1][visible_limb], points[2][visible_limb], marker='o',s=4, color='blue')\n",
    "        return ax, line\n",
    "    elif len(points) == 28:\n",
    "        points = np.array(points).T\n",
    "        for num in range(len(reduced_limb_ranges)):\n",
    "            visible_limb = []\n",
    "            for x in range(reduced_limb_ranges[num][0],reduced_limb_ranges[num][1]):\n",
    "                if vis[x]== 1:\n",
    "                    visible_limb.append(x) \n",
    "            if prediction:\n",
    "                line, = ax.plot(points[0][visible_limb], points[1][visible_limb], points[2][visible_limb],'--', alpha=0.7, label='Prediction', color='red')\n",
    "                ax.scatter(points[0][visible_limb], points[1][visible_limb], points[2][visible_limb], marker='X',s=10, color='red')\n",
    "            else:\n",
    "                line, = ax.plot(points[0][visible_limb], points[1][visible_limb], points[2][visible_limb], label='Actual', color='blue')\n",
    "                ax.scatter(points[0][visible_limb], points[1][visible_limb], points[2][visible_limb], marker='o',s=4, color='blue')\n",
    "        return ax, line\n",
    "\n",
    "def set_axes_equal(ax):\n",
    "    # workaround, as matplotlib's 3D plot has no option for equisised axes (10/2021)\n",
    "    x_limits = ax.get_xlim3d()\n",
    "    y_limits = ax.get_ylim3d()\n",
    "    z_limits = ax.get_zlim3d()\n",
    "\n",
    "    x_range = abs(x_limits[1] - x_limits[0])\n",
    "    x_middle = np.mean(x_limits)\n",
    "    y_range = abs(y_limits[1] - y_limits[0])\n",
    "    y_middle = np.mean(y_limits)\n",
    "    z_range = abs(z_limits[1] - z_limits[0])\n",
    "    z_middle = np.mean(z_limits)\n",
    "\n",
    "    plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "\n",
    "    ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "    ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "    ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/single_sungaya/label_names.txt'\n",
    "\n",
    "with open(file_path) as file:\n",
    "    label_names = file.readlines()\n",
    "    label_names = [item.rstrip() for item in label_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"../data/single_sungaya/\"\n",
    "out_df = pd.read_hdf(os.path.join(target_dir, \"Data_3D_Pose.hdf5\"))\n",
    "reduceKeypoints = True\n",
    "sungaya_dataset = src.bug_dataset.BugDataset(df=out_df,\n",
    "                             root_dir=target_dir, reduced=reduceKeypoints, transform=transforms.Compose([\n",
    "                                src.bug_dataset.ToTensor()\n",
    "                                   ]))\n",
    "train_split = 0.7\n",
    "valid_split = 0.1\n",
    "train_size = int(train_split * len(sungaya_dataset))\n",
    "valid_size = int(valid_split * len(sungaya_dataset))\n",
    "test_size = len(sungaya_dataset) - (train_size+valid_size)\n",
    "_,_ ,test_dataset = torch.utils.data.random_split(sungaya_dataset, [train_size, valid_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 128\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PoseResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (deconv_layers): Sequential(\n",
       "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       "  (final_layer): Conv2d(256, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))   \n",
    "\n",
    "# Load Models\n",
    "num_layers = 18\n",
    "resnet_spec = {18: (BasicBlock, [2, 2, 2, 2]),\n",
    "               34: (BasicBlock, [3, 4, 6, 3]),\n",
    "               50: (Bottleneck, [3, 4, 6, 3]),\n",
    "               101: (Bottleneck, [3, 4, 23, 3]),\n",
    "               152: (Bottleneck, [3, 8, 36, 3])}\n",
    "if reduceKeypoints:\n",
    "    block_class, layers = resnet_spec[num_layers]\n",
    "    model_2d = PoseResNet(block_class, layers, 28)\n",
    "\n",
    "else:\n",
    "    block_class, layers = resnet_spec[num_layers]\n",
    "    model_2d = PoseResNet(block_class, layers, 62)\n",
    "\n",
    "model_2d = model_2d.to(device)\n",
    "path = \"../models/2d_horiz_vertical_flip/simple_2d_model_20220321-195443_128\"\n",
    "model_2d.load_state_dict(torch.load(path))\n",
    "model_2d.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel(\n",
       "  (w1): Linear(in_features=56, out_features=1024, bias=True)\n",
       "  (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear_stages): ModuleList(\n",
       "    (0): Linear(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (w1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (w2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (batch_norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Linear(\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (w1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (batch_norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (w2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (batch_norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (w2): Linear(in_features=1024, out_features=28, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if reduceKeypoints:\n",
    "    model_lift = LinearModel(2*28,28,1024) # Linear size 512 looked promising 256 too small\n",
    "else:\n",
    "    model_lift = LinearModel(2*62,62,512)\n",
    "\n",
    "model_lift = model_lift.to(device)\n",
    "path = \"../models/lifting_depth/lifting_depth_model_20220323-150326_128\"\n",
    "model_lift.load_state_dict(torch.load(path))\n",
    "model_lift.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(model_2d,model_lift, ds, frac_noise=0.0,thr=15.0):\n",
    "    test_epoch_acc, test_epoch_loss = 0, 0\n",
    "    test_epoch_acc_kps = [0]*28\n",
    "    batches = 0\n",
    "    # Code get the first batch of results\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(ds, desc=\"2-Stage Experiment\"):\n",
    "            input = data['image'].to(device, dtype=torch.float)\n",
    "            y = data['key_points_3D'][:,:,2].to(device)\n",
    "            mask = data['visibility'].to(device)\n",
    "\n",
    "            \n",
    "            \n",
    "            pred_heatmap =  model_2d(input)\n",
    "            kps2d,_ = get_max_preds(pred_heatmap.detach().cpu().numpy())\n",
    "\n",
    "            # Add noise to the 2d input add guassian noise to determine how sensitve the model is that will give us a reason to why the model is working so badly\n",
    "            kps2d = kps2d.reshape((len(kps2d), 56))\n",
    "            for bat in range(len(kps2d)):\n",
    "                for col in range(len(kps2d[bat])):\n",
    "                    mean = kps2d[:,col].mean()\n",
    "                    noise = np.random.normal(0,frac_noise*mean,len(kps2d[:,col]))\n",
    "                    kps2d[:,col] += noise\n",
    "\n",
    "            pred_lift = model_lift(torch.from_numpy(kps2d).to(device))\n",
    "            \n",
    "            test_loss = torch.mean(((pred_lift - y)*mask)**2) # \n",
    "                    \n",
    "            test_acc,test_acc_kps = keypoint_depth_pck(pred_lift.detach().cpu().numpy(),\n",
    "                                        y.detach().cpu().numpy(), \n",
    "                                        mask.detach().cpu().numpy(),\n",
    "                                        sungaya_dataset.std_3d[:,2], sungaya_dataset.means_3d[:,2], threshold = thr)\n",
    "            test_epoch_loss += test_loss.item()\n",
    "            test_epoch_acc += test_acc\n",
    "            test_epoch_acc_kps += test_acc_kps\n",
    "            batches +=1\n",
    "            \n",
    "    test_acc = test_epoch_acc/batches\n",
    "    test_acc_kps = test_epoch_acc_kps/batches\n",
    "    test_loss = test_epoch_loss/batches\n",
    "    return test_acc, test_loss, test_acc_kps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab89b1abb0904a468bbb92088a975306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05423390679061413\n",
      "301.1167200554448\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss, test_acc_kps = test_func(model_2d, model_lift, test_dataloader)\n",
    "print(test_acc)\n",
    "print(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f421c59fb54846d0878cdc77731f327b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots the accuracies per KP at normal threshold\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(11,8))\n",
    "\n",
    "ax.barh(np.array(label_names)[sungaya_dataset.reduced_kp], test_acc_kps, linewidth =5, edgecolor=\"white\")\n",
    "ax.invert_yaxis()\n",
    "# ax.set(xlim=(-1, 8), xticks=np.arange(-1, 8),\n",
    "#        ylim=(0, 1), yticks=np.arange(0, 0.5))\n",
    "plt.title(\"2 Stage PCK Accuracies for every Keypoint on Synthetic Data\")\n",
    "\n",
    "plt.xlabel(\"Accuracies\")\n",
    "plt.ylabel(\"Keypoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310f0af5b8c84f28891e118066b30829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67bb1d896734d269d3657fa091f2bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da33508a88145699355cd5391b2a9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9131969ef634ab49a09c97990e4bd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4c4da7d1664e5582dc745a58aebc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584dfea879234d508ea0cf58d2d82d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c7450e3fe6432a8bf6856fbcd94372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d919464a1959400fb77d379223dc5280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to do an experiment where we vary the threshold value on the test set\n",
    "thresholds = [0.01 ,0.1, 1.0, 2.5, 5.0, 10.0, 15.0, 20.0]\n",
    "accs = []\n",
    "losses = []\n",
    "acc_kps = []\n",
    "for thr in thresholds:\n",
    "    test_acc, test_loss, test_acc_kps = test_func(model_2d, model_lift, test_dataloader, thr=thr)\n",
    "    accs.append(test_acc)\n",
    "    losses.append(test_loss)\n",
    "    acc_kps.append(test_acc_kps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2999218c67f64458831e8269eef38ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots PCK for different thresholds\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(list(map(str,thresholds)), accs, width=1, edgecolor=\"white\", linewidth=5)\n",
    "\n",
    "plt.title(\"2 Stage PCK Accuracies at Varying Thresholds on Synthetic Data\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a25923521334f69847798ca3530a7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712d26645e344cad82ca0ae8e66c7ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530eb49e5c454988adbc24b8559ce651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7ae610ed414a1682b54a81ba9cb4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3629a7444a574c17b83fb604a79a00b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fee587774544c687987040725385b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2-Stage Experiment:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to do an experiment where we vary the amount of noise value on the test set\n",
    "frac_noise = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]\n",
    "frac_noise = [0.01,0.001,0.001,0.0001,0.00001,0.000001]\n",
    "accs = []\n",
    "losses = []\n",
    "acc_kps = []\n",
    "for frac in frac_noise:\n",
    "    test_acc, test_loss, test_acc_kps = test_func(model_2d, model_lift, test_dataloader, frac_noise =frac)\n",
    "    accs.append(test_acc)\n",
    "    losses.append(test_loss)\n",
    "    acc_kps.append(test_acc_kps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcc377870e743fcbd59f6ecb5e6ab76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots PCK for different noise \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(list(map(str,frac_noise)), accs, width=1, edgecolor=\"white\", linewidth=5)\n",
    "\n",
    "plt.title(\"2 Stage PCK Accuracies with Varying Noises on Synthetic Data\")\n",
    "plt.xlabel(\"Percentage Noise\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02436a74caeae32c2ef71ab76e7f86264abcd5d5376c75c1970567ef169e8611"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
